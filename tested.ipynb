{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import midi_to_song, log_midis\n",
    "from loss import kl_normal, log_bernoulli_with_logits\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "from dataloader import MusicDataset\n",
    "from model import DVAE \n",
    "from einops import repeat, rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = OmegaConf.load('config.yaml')\n",
    "\n",
    "model = DVAE(input_dim=config.model.input_dim, \n",
    "                hidden_dim=config.model.hidden_dim,\n",
    "                hidden_dim_em=config.model.hidden_dim_em, \n",
    "                hidden_dim_tr=config.model.hidden_dim_tr, \n",
    "                latent_dim=config.model.latent_dim,\n",
    "                dropout=config.model.dropout,\n",
    "                combiner_type=config.model.combiner_type,\n",
    "                rnn_type=config.model.rnn_type).to(device)\n",
    "\n",
    "dataset = MusicDataset(config.dataset, split=config.sample.split)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "#load weights\n",
    "ckpt_path = config.test.ckpt_path\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "a = 0\n",
    "b = 0\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_lengths: tensor([129,  65])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():   \n",
    "    for j, (encodings, sequence_lengths) in enumerate(dataloader):\n",
    "        \n",
    "        print(f'sequence_lengths: {sequence_lengths}')\n",
    "        encodings = encodings.to(device)\n",
    "        sequence_lengths = sequence_lengths.to(device)\n",
    "        \n",
    "        x_hat, mus_inference, sigmas_inference, mus_generator, sigmas_generators = model(encodings)\n",
    "        \n",
    "        #get loss with only sum over latent dim dimension\n",
    "        reconstruction_loss = log_bernoulli_with_logits(encodings, x_hat, sequence_lengths, T_reduction='none') \n",
    "        kl_loss = kl_normal(mus_inference, \n",
    "                            sigmas_inference, \n",
    "                            mus_generator, \n",
    "                            sigmas_generators, \n",
    "                            sequence_lengths,\n",
    "                            T_reduction='none')\n",
    "        \n",
    "        kl_loss = kl_loss.mean(-1) #sum over T\n",
    "        reconstruction_loss = reconstruction_loss.mean(-1) #sum over T\n",
    "        \n",
    "        #for a: #importance sampling\n",
    "        z, mu_q, var_q = model.encoder(encodings)\n",
    "        bs = encodings.shape[0]\n",
    "        max_sequence_length = encodings.shape[1]\n",
    "        loss_s = torch.zeros(bs)\n",
    "        all_exponent_args = []\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_lengths.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction_loss: 10.08811092376709\n",
      "kl_loss: 1.011856198310852\n"
     ]
    }
   ],
   "source": [
    "reconstruction_loss = reconstruction_loss[0]\n",
    "kl_loss = kl_loss[0]\n",
    "\n",
    "print(f'reconstruction_loss: {reconstruction_loss}')\n",
    "print(f'kl_loss: {kl_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.1000)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nelbo_matrix = reconstruction_loss + kl_loss\n",
    "nelbo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2494.1921)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nelbo_matrix = nelbo_matrix.sum(-1) #sum over batch_size\n",
    "# nelbo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths_sum = sequence_lengths[0]\n",
    "sequence_lengths_sum\n",
    "nelbo_b = nelbo_matrix / sequence_lengths_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0860)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nelbo_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for b:\n",
    "nelbo_matrix = reconstruction_loss + kl_loss\n",
    "nelbo_matrix = nelbo_matrix.sum(-1) #sum over batch_size\n",
    "sequence_lengths_sum = sequence_lengths.sum(-1)\n",
    "nelbo_b = nelbo_matrix / sequence_lengths_sum\n",
    "b += nelbo_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.8567)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an rnn with 2 layers\n",
    "rnn = torch.nn.RNN(input_size=88, hidden_size=600, num_layers=2, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(2, 10, 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = rnn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 600])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 10, 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 88])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([2, 129, 88])\n",
      "sequence_lengths: torch.Size([2])\n",
      "encodings: torch.Size([1, 129, 88])\n",
      "sequence_lengths: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (encodings, sequence_lengths) in enumerate(dataloader):\n",
    "    print(f'encodings: {encodings.shape}')\n",
    "    print(f'sequence_lengths: {sequence_lengths.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset.read_pickle_from_url('data/jsb_chorales.pickle')['test'][0] #84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = [a]\n",
    "all_music_one_hot_list = []\n",
    "note_range = 88\n",
    "min_note = 21\n",
    "sequence_lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for music in split_data:\n",
    "    one_hot_matrix = np.zeros((len(music), note_range), dtype=int)\n",
    "    \n",
    "    for row_index, keys in enumerate(music):\n",
    "\n",
    "        for note in keys:\n",
    "            one_hot_matrix[row_index, note - min_note] = 1  \n",
    "            \n",
    "    all_music_one_hot_list.append(one_hot_matrix)\n",
    "    sequence_lengths.append(len(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = all_music_one_hot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "length: 88\n"
     ]
    }
   ],
   "source": [
    "length = len(sample)\n",
    "\n",
    "print(f'sample: {sample}')\n",
    "print(f'length: {length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_packed[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_packed = pack_padded_sequence(encodings, sequence_lengths, batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(input_size=88, hidden_size=600, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn(x_packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([34])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(34).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'batch_sizes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/fedra/Desktop/re-SIN-for-NSSM/tested.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fedra/Desktop/re-SIN-for-NSSM/tested.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m out, _ \u001b[39m=\u001b[39m pad_packed_sequence(out, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/utils/rnn.py:325\u001b[0m, in \u001b[0;36mpad_packed_sequence\u001b[0;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpad_packed_sequence\u001b[39m(\n\u001b[1;32m    269\u001b[0m     sequence: PackedSequence,\n\u001b[1;32m    270\u001b[0m     batch_first: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m     padding_value: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m,\n\u001b[1;32m    272\u001b[0m     total_length: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m    274\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Pads a packed batch of variable length sequences.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[39m    It is an inverse operation to :func:`pack_padded_sequence`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m \n\u001b[1;32m    324\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m     max_seq_length \u001b[39m=\u001b[39m sequence\u001b[39m.\u001b[39;49mbatch_sizes\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m    326\u001b[0m     \u001b[39mif\u001b[39;00m total_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m         \u001b[39mif\u001b[39;00m total_length \u001b[39m<\u001b[39m max_seq_length:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'batch_sizes'"
     ]
    }
   ],
   "source": [
    "out, _ = pad_packed_sequence(out, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 88])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodings shape: torch.Size([1, 129, 88])\n",
      "encodings shape: torch.Size([1, 65, 88])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 129, 88] at entry 0 and [1, 65, 88] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/fedra/Desktop/re-SIN-for-NSSM/tested.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fedra/Desktop/re-SIN-for-NSSM/tested.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (encodings, sequence_lengths) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fedra/Desktop/re-SIN-for-NSSM/tested.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mencodings: \u001b[39m\u001b[39m{\u001b[39;00mencodings\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fedra/Desktop/re-SIN-for-NSSM/tested.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39msequence_lengths: \u001b[39m\u001b[39m{\u001b[39;00msequence_lengths\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 129, 88] at entry 0 and [1, 65, 88] at entry 1"
     ]
    }
   ],
   "source": [
    "for i, (encodings, sequence_lengths) in enumerate(dataloader):\n",
    "    print(f'encodings: {encodings.shape}')\n",
    "    print(f'sequence_lengths: {sequence_lengths.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
